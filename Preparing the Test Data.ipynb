{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The purpose of this notebook will be to modify the test data to prepare it for the model.\n",
    "\n",
    "I'll be reusing the code from the \"Data Wrangling\" notebook to modify the test data similarly to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import LabelEncoder from sklearn.preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the test_data\n",
    "test_df = pd.read_csv('./Raw_Data/application_test.csv', encoding='utf-8')\n",
    "\n",
    "# Import the pre-merge training data\n",
    "train_df = pd.read_csv('.\\Intermediate_Data\\\\intermediate_train.csv', encoding='utf-8', index_col=0)\n",
    "\n",
    "# Import the intermediate bureau data\n",
    "bureau_df = pd.read_csv('.\\Intermediate_Data\\\\cleaned_bureau.csv', encoding='utf-8',index_col=0)\n",
    "\n",
    "# Import the intermediate credit data\n",
    "credit_df = pd.read_csv('.\\Intermediate_Data\\\\cleaned_credit.csv', encoding='utf-8', index_col=0)\n",
    "\n",
    "# Import the merge intermediate previous applications and installment adata\n",
    "prev_instal_df = pd.read_csv('.\\Intermediate_Data\\\\cleaned_prev_instal.csv', encoding='utf-8', index_col=0)\n",
    "\n",
    "# Import the merged training data for comparison\n",
    "merged_train_df = pd.read_csv('./Intermediate_Data/merged_train.csv', encoding='utf-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the same feature engineering as training data\n",
    "EXT_SOURCE = ['EXT_SOURCE_3', 'EXT_SOURCE_2', 'EXT_SOURCE_1']\n",
    "\n",
    "DOCUMENTS = ['FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', \n",
    "             'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9',\n",
    "             'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13',\n",
    "             'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17',\n",
    "             'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21']\n",
    "\n",
    "ENQUIRIES = ['AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
    "             'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR']\n",
    "\n",
    "# Define a valid mobile column. This will account if a mobile phone was provided and if it was\n",
    "# reachable\n",
    "test_df['VALID_MOBILE'] = (test_df['FLAG_MOBIL'] + test_df['FLAG_CONT_MOBILE']) == 2\n",
    "\n",
    "# Define a new column for quality of the region and quality of housing relative to region\n",
    "# I'll put a higher weight on the quality of the region, and a lower weight on the relative rating (3.33 - 10)\n",
    "test_df['HOUSING'] = (test_df['REGION_RATING_CLIENT'] * 3) + (test_df['REGION_RATING_CLIENT_W_CITY'] / 3)\n",
    "\n",
    "# Define the new documents column. Take the sum to count how many documents were provided.\n",
    "test_df['DOCUMENTS'] = test_df[DOCUMENTS].sum(1)\n",
    "\n",
    "# Define the new enquiries column. Take the sum to count total enquiries to the Credit Bureau over\n",
    "# the past year.\n",
    "test_df['ENQUIRIES'] = test_df[ENQUIRIES].sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex the test data with columns in the training data\n",
    "new_test = test_df.reindex(train_df.columns, axis=1)\n",
    "\n",
    "# Remove the TARGET column\n",
    "new_test.drop('TARGET', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AMT_ANNUITY', 'NAME_TYPE_SUITE', 'OWN_CAR_AGE', 'DEF_30_CNT_SOCIAL_CIRCLE', 'EXT_SOURCE_3', 'EXT_SOURCE_2', 'EXT_SOURCE_1']\n"
     ]
    }
   ],
   "source": [
    "# Let's define a function that takes a dataframe and returns all the columns with null values\n",
    "def NULL_COLUMNS(df):\n",
    "    '''Returns all the columns with null values'''\n",
    "    null_list = []\n",
    "    for col in df:\n",
    "        if sum(df[col].isnull()) != 0:\n",
    "            null_list.append(col)\n",
    "    return null_list\n",
    "\n",
    "# Let's check which columns have null values in the training data\n",
    "print (NULL_COLUMNS(new_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the null values accordingly\n",
    "\n",
    "# Fill AMT_ANNUITY with 0's\n",
    "new_test['AMT_ANNUITY'].fillna(new_test['AMT_ANNUITY'].median(), inplace=True)\n",
    "\n",
    "# Fill AMT_GOODS_PRICE with 0's\n",
    "new_test['AMT_GOODS_PRICE'].fillna(new_test['AMT_GOODS_PRICE'].median(), inplace=True)\n",
    "\n",
    "# Convert NAME_TYPE_SUITE to a binary\n",
    "new_test['NAME_TYPE_SUITE'].fillna(value='Unaccompanied', inplace=True)\n",
    "suite_convert = (lambda x: 0 if str(x) == 'Unaccompanied' else 1)\n",
    "new_test['NAME_TYPE_SUITE'] = train_df['NAME_TYPE_SUITE'].apply(suite_convert)\n",
    "\n",
    "# Fill OWN_CAR_AGE with 0's\n",
    "new_test['OWN_CAR_AGE'].fillna(0, inplace=True)\n",
    "\n",
    "# Fill DEF_30_CNT_SOCIAL_CIRCLE with -1's\n",
    "new_test['DEF_30_CNT_SOCIAL_CIRCLE'].fillna(-1, inplace=True)\n",
    "\n",
    "for EXT_SOURCE in NULL_COLUMNS(new_test):\n",
    "    new_test[EXT_SOURCE].fillna(new_test[EXT_SOURCE].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check to make sure there are no more null values\n",
    "assert NULL_COLUMNS(new_test) == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address the outliers and odd values similarly\n",
    "\n",
    "# Replace the 365243's with 0's\n",
    "new_test['DAYS_EMPLOYED'].replace(to_replace=365243, value=0, inplace=True)\n",
    "\n",
    "# Replace the extreme value of 34.0 with 9.0\n",
    "new_test.loc[train_df['SK_ID_CURR'] == 272071, 'DEF_30_CNT_SOCIAL_CIRCLE'] = 9.0\n",
    "\n",
    "# Replace the extreme value of 262.0 with 9.0\n",
    "new_test.loc[train_df['SK_ID_CURR'] == 377322, 'ENQUIRIES'] = 32.0\n",
    "\n",
    "# Replace the highest value with the second highest value.\n",
    "new_test.loc[train_df['SK_ID_CURR'] == 377322, 'AMT_INCOME_TOTAL'] = 18000090.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the domain features as seen i\n",
    "\n",
    "# Create the variable PAYMENT_RATE\n",
    "new_test['PAYMENT_RATE'] = new_test['AMT_ANNUITY'] / new_test['AMT_CREDIT']\n",
    "\n",
    "# Create the variable ANNUITY_INCOME_PERC\n",
    "new_test['ANN_INCOME_PERC'] = new_test['AMT_ANNUITY'] / new_test['AMT_INCOME_TOTAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "# Test data should have all columns in train data besides target, and an additional 2 defined columns here\n",
    "assert (len(new_test.columns) == (len(train_df.drop('TARGET', axis=1).columns) + 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the test data and bureau data\n",
    "test_bureau = new_test.merge(bureau_df, how='left', on='SK_ID_CURR')\n",
    "\n",
    "# Sanity check to make sure no unique SK_ID_CURR from the training data is lost\n",
    "assert new_test['SK_ID_CURR'].nunique() == test_bureau['SK_ID_CURR'].nunique()\n",
    "\n",
    "# Sanity check to check if all columns were added\n",
    "assert len(test_bureau.columns) == (len(new_test.columns) + len(bureau_df.columns) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LAST_ONGOING_APP',\n",
       " 'TOTAL_ONGOING',\n",
       " 'ONGOING_CREDIT',\n",
       " 'TOTAL_CLOSED',\n",
       " 'CLOSED_CREDIT']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for null values\n",
    "NULL_COLUMNS(test_bureau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill np.nan in LAST_ONGOING_APP with -1's\n",
    "test_bureau['LAST_ONGOING_APP'].fillna(-1, inplace=True)\n",
    "\n",
    "# Fill np.nan in TOTAL_ONGOING with 0's\n",
    "test_bureau['TOTAL_ONGOING'].fillna(0, inplace=True)\n",
    "\n",
    "# Fill np.nan in ONGOING_CREDIT with 0's\n",
    "test_bureau['ONGOING_CREDIT'].fillna(0, inplace=True)\n",
    "\n",
    "# Fill np.nan in TOTAL_CLOSED  with 0's\n",
    "test_bureau['TOTAL_CLOSED'].fillna(0, inplace=True)\n",
    "\n",
    "# Fill np.nan in CLOSED_CREDIT with 0's\n",
    "test_bureau['CLOSED_CREDIT'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check to make sure there are no more null values\n",
    "assert NULL_COLUMNS(test_bureau) == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the test_bureau with credit_df using a left merge\n",
    "Tr_Br_Cr = test_bureau.merge(credit_df, how='left', on='SK_ID_CURR')\n",
    "\n",
    "# Sanity check to make sure no unique SK_ID_CURR from the training data is lost\n",
    "assert new_test['SK_ID_CURR'].nunique() == Tr_Br_Cr['SK_ID_CURR'].nunique()\n",
    "\n",
    "# Sanity check to check if all columns were added\n",
    "assert len(Tr_Br_Cr.columns) == (len(test_bureau.columns) + len(credit_df.columns) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MONTHS_BALANCE', 'SK_DPD', 'NET_PAID']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for null values\n",
    "NULL_COLUMNS(Tr_Br_Cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the np.nan's in MONTHS_BALANCE with 0's\n",
    "Tr_Br_Cr['MONTHS_BALANCE'].fillna(0, inplace=True)\n",
    "\n",
    "# Fill the np.nan's in SK_DPD with 0's\n",
    "Tr_Br_Cr['SK_DPD'].fillna(0, inplace=True)\n",
    "\n",
    "# Fill the np.nan's in NET_PAID with 0's\n",
    "Tr_Br_Cr['NET_PAID'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check to make sure there are no more null values\n",
    "assert NULL_COLUMNS(Tr_Br_Cr) == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the AMT_CREDIT column to PREV_CREDIT to avoid overlapping columns\n",
    "prev_instal_df.columns = ['SK_ID_CURR', 'PREV_CREDIT', 'DAYS_DECISION',\n",
    "                         'DAYS_TERMINATION', 'NET_PAYMENT', 'PAYMENT_TIME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the train_bureau with credit_df using a left merge\n",
    "merged_test = Tr_Br_Cr.merge(prev_instal_df, how='left', on='SK_ID_CURR')\n",
    "\n",
    "# Sanity check to make sure no unique SK_ID_CURR from the training data is lost\n",
    "assert new_test['SK_ID_CURR'].nunique() == merged_test['SK_ID_CURR'].nunique()\n",
    "\n",
    "# Sanity check to check if all columns were added\n",
    "assert len(merged_test.columns) == (len(Tr_Br_Cr.columns) + len(prev_instal_df.columns) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PREV_CREDIT',\n",
       " 'DAYS_DECISION',\n",
       " 'DAYS_TERMINATION',\n",
       " 'NET_PAYMENT',\n",
       " 'PAYMENT_TIME']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for null values\n",
    "NULL_COLUMNS(merged_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the np.nan's in PREV_CREDIT with 0's\n",
    "merged_test['PREV_CREDIT'].fillna(0, inplace=True)\n",
    "\n",
    "# Fill the np.nan's in DAYS_DECISION with 0's\n",
    "merged_test['DAYS_DECISION'].fillna(0, inplace=True)\n",
    "\n",
    "# Fill the np.nan's in DAYS_TERMINATION with 0's\n",
    "merged_test['DAYS_TERMINATION'].fillna(0, inplace=True)\n",
    "\n",
    "# Fill the np.nan's in NET_PAYMENT with 0's\n",
    "merged_test['NET_PAYMENT'].fillna(0, inplace=True)\n",
    "\n",
    "# Fill the np.nan's in PAYMENT_TIME with 0's\n",
    "merged_test['PAYMENT_TIME'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the AMT_ANNUITY and AMT_GOODS_PRICE columns\n",
    "# merged_test.drop(['AMT_ANNUITY', 'AMT_GOODS_PRICE'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data should have all columns in train data besides target\n",
    "assert (list(merged_test.columns) == list(merged_train_df.drop('TARGET', axis=1).columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_test = merged_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new lists with binary and non-binary categories. Leave out the SK_ID_CURR and TARGET from the columns\n",
    "\n",
    "# Define the binary categories\n",
    "BINARY_CAT = ['NAME_CONTRACT_TYPE', 'FLAG_OWN_REALTY', 'NAME_TYPE_SUITE', 'REG_REGION_NOT_LIVE_REGION', 'VALID_MOBILE']\n",
    "\n",
    "# Define the non-binary categories\n",
    "NON_BIN_CAT = ['NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE']\n",
    "\n",
    "# Instantiate the LabelEncoder\n",
    "labEncode = LabelEncoder()\n",
    "\n",
    "# Convert the dtype category columns in BINARY_CAT to numeric data using LabelEncoder\n",
    "for col in BINARY_CAT:\n",
    "    if encoding_test[col].dtype != 'int64':\n",
    "        encoding_test[col] = labEncode.fit_transform(encoding_test[col])\n",
    "        \n",
    "# Convert all column sin NON_BIN_CAT to numeric data using LabelEncoder\n",
    "for col in NON_BIN_CAT:\n",
    "    encoding_test[col] = labEncode.fit_transform(encoding_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the DAYS_DECISION column\n",
    "encoding_test.drop(['DAYS_DECISION', 'AMT_GOODS_PRICE', 'CNT_CHILDREN'], axis=1, inplace=True)\n",
    "\n",
    "# Sanity check to make sure the column has been dropped\n",
    "assert 'DAYS_DECISION' not in encoding_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "\n",
    "# Ensure no rows were lost\n",
    "assert len(encoding_test) == len(merged_test)\n",
    "\n",
    "# Ensure no additional columns were removed besides DAYS_DECISIONS\n",
    "assert len(encoding_test.columns) == len(merged_test.columns) - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the dataframe to a csv for use in another notebook\n",
    "encoding_test.to_csv('./Intermediate_Data/encoded_test.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
